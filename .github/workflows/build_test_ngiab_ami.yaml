name: Test Datastream NGIAB AMI Integration

on:
  workflow_dispatch:
    inputs:
      ds_tag:
        description: 'Datastream Docker tag'
        required: false
        default: 'latest'
        type: string
      fp_tag:
        description: 'Forcing Processor Docker tag'
        required: false
        default: 'latest'
        type: string
      ngiab_tag:
        description: 'NGIAB Docker tag'
        required: false
        default: 'latest'
        type: string


permissions:
  contents: write

jobs:
  test-ngiab-dev-integration:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3      

    - name: Configure AWS
      run: |
        aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws configure set region us-east-1    
        export AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
        export AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Build docker containers
      run : |        
        export ARCH=$(uname -m)
        docker compose -f docker/docker-compose.yml build datastream-deps
        docker compose -f docker/docker-compose.yml build datastream
        docker compose -f docker/docker-compose.yml build forcingprocessor
        docker images


  test-ngiab-ds-integration-with-workflow-tags:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4  # Updated version

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3      

    - name: Display trigger information
      run: |
        echo "Workflow triggered manually with versions:"
        echo "DS_TAG: ${{ inputs.ds_tag }}"
        echo "FP_TAG: ${{ inputs.fp_tag }}"
        echo "NGIAB_TAG: ${{ inputs.ngiab_tag }}"

    - name: Configure AWS
      run: |
        aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws configure set region us-east-1    
        export AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
        export AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}

    - name: Base test and NWM_RETRO_V3
      run: |        
        sudo rm -rf $(pwd)/data/datastream_test
        # Export version tags for datastream script
        export DS_TAG="${{ inputs.ds_tag }}"
        export FP_TAG="${{ inputs.fp_tag }}"
        export NGIAB_TAG="${{ inputs.ngiab_tag }}"        
        ./scripts/datastream -s 202006200100 -e 202006200200 -C NWM_RETRO_V3 -d $(pwd)/data/datastream_test -g https://ngen-datastream.s3.us-east-2.amazonaws.com/palisade.gpkg -R $(pwd)/configs/ngen/realization_sloth_nom_cfe_pet.json


  bump-version:
    needs: [test-ngiab-dev-integration, test-ngiab-ds-integration-with-workflow-tags]
    if: always() && (needs.test-ngiab-dev-integration.result == 'success' || needs.test-ngiab-ds-integration-with-workflow-tags.result == 'success')
    runs-on: ubuntu-latest
    outputs:
      new_version: ${{ steps.version.outputs.new_version }}
    steps:
    - uses: actions/checkout@v4
    - name: Bump version
      id: version
      run: |
        current_version=$(grep 'datastream-ami-version:' ./ami_version.yml | sed 's/.*"\([0-9]*\)\.\([0-9]*\)\.\([0-9]*\)".*/\1.\2.\3/')
        major=$(echo $current_version | cut -d. -f1)
        minor=$(echo $current_version | cut -d. -f2)
        patch=0
        new_minor=$((minor + 1))
        new_version="$major.$new_minor.$patch"
        
        echo "new_version=$new_version" >> "$GITHUB_OUTPUT"
        
        sed -i "s/datastream-ami-version: \"[0-9]*\.[0-9]*\.[0-9]*\"/datastream-ami-version: \"$new_version\"/" ./ami_version.yml
        
        git config user.name "bot"
        git config user.email "bot@github.com"
        git add ./ami_version.yml
        git commit -m "bump version to $new_version"
        git push

  build-test-push-ami:
    needs: bump-version
    runs-on: ubuntu-latest
    outputs:
      ds_tag: ${{ inputs.ds_tag }}
      fp_tag: ${{ inputs.fp_tag }}
      ngiab_tag: ${{ inputs.ngiab_tag }}
      ds_ami_version: ${{ steps.changes.outputs.ds_ami_version }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref_name }}
          fetch-depth: 1

      - name: Install yq
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq

      - name: Get version info
        id: changes
        shell: bash
        run: |
          set -euo pipefail
          CURRENT_DS_AMI=$(yq -r e '."datastream-ami-version"' ami_version.yml)
          echo "Using Docker tags:"
          echo "DS_TAG=${{ inputs.ds_tag }}"
          echo "FP_TAG=${{ inputs.fp_tag }}"
          echo "NGIAB_TAG=${{ inputs.ngiab_tag }}"
          echo "ds_ami_version=$CURRENT_DS_AMI" >> "$GITHUB_OUTPUT"

      - name: Configure AWS
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region us-east-1        

      - name: Build AMI on AWS (N Virginia)
        run: |
          ami_version=datastream-"${{ steps.changes.outputs.ds_ami_version }}"
          echo "Building AMI: $ami_version"
          echo "With Docker tags: DS=${{ inputs.ds_tag }}, FP=${{ inputs.fp_tag }}, NGIAB=${{ inputs.ngiab_tag }}"
          export AMI_NAME="$ami_version"
          export DS_TAG="${{ inputs.ds_tag }}"
          export FP_TAG="${{ inputs.fp_tag }}"
          export NGIAB_TAG="${{ inputs.ngiab_tag }}"
          chmod +x scripts/create_ami.sh
          ./scripts/create_ami.sh
          
  get-ami-info:
    needs: build-test-push-ami
    runs-on: ubuntu-latest
    outputs:
      ami_id: ${{ steps.get-ami.outputs.ami_id }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Get AMI ID
        id: get-ami
        run: |
          ami_name="datastream-${{ needs.build-test-push-ami.outputs.ds_ami_version }}"
          ami_id=$(aws ec2 describe-images \
            --owners self \
            --filters "Name=name,Values=$ami_name" \
            --query 'Images[0].ImageId' \
            --output text \
            --region us-east-1)
          echo "ami_id=$ami_id" >> "$GITHUB_OUTPUT"


  test-research-datastream-forcingprocessing:
    needs: [build-test-push-ami, get-ami-info]
    runs-on: ubuntu-latest
    env:
      DATE: 20250801
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          pip install --upgrade awscli


      - name: Download and modify execution JSON
        run: |
          cd research_datastream/terraform_community
          
          # Use the AMI ID from the get-ami-info job
          ami_id="${{ needs.get-ami-info.outputs.ami_id }}"
          ami_name="datastream-${{ needs.build-test-push-ami.outputs.ds_ami_version }}"
          
          # Verify AMI ID is valid
          if [ "$ami_id" == "None" ] || [ "$ami_id" == "" ] || [ "$ami_id" == "null" ]; then
            echo " Invalid AMI ID: $ami_id"
            echo "Available AMIs:"
            aws ec2 describe-images --owners self --query 'Images[*].[Name,ImageId]' --output table --region us-east-1
            exit 1
          fi
          
          echo " Using AMI ID: $ami_id for AMI name: $ami_name"
          
          # Download the proven working execution JSON from S3
          aws s3 cp s3://ciroh-community-ngen-datastream/v2.2/ngen.${{ env.DATE }}/forcing_short_range/00/metadata/execution.json execution.json
          
          # Replace the working AMI with custom one
          jq --arg ami "$ami_id" '.instance_parameters.ImageId = $ami' execution.json > execution_updated.json
          mv execution_updated.json execution.json
          echo $ami_id
          # Replace Docker image tags with specific versions
          sed -i "s|DS_TAG=1.0.1|DS_TAG=${{ needs.build-test-push-ami.outputs.ds_tag }}|g" execution.json
          sed -i "s|NGIAB_TAG=v0.0.0|NGIAB_TAG=${{ needs.build-test-push-ami.outputs.ngiab_tag }}|g" execution.json
          cat execution.json
          # Show key info (avoid dumping entire JSON to logs)
          echo "AMI replacement completed for VPU ${{ env.VPU }}"
          echo "New AMI ID: $(jq -r '.instance_parameters.ImageId' execution.json)"
          
          jq --arg DATE "${{ env.DATE }}" 'del(.instance_parameters.MaxCount, .instance_parameters.MinCount, .instance_parameters.InstanceId, .t0, .ii_s3_object_checked, .retry_attempt, .region) | .instance_parameters.TagSpecifications = [{"ResourceType": "instance", "Tags": [{"Key": "Project", "Value": "fp_test_git_actions"}]}] | .commands |= map(sub("--s3_prefix(?i) \\K[^ ]+"; "tests/short_range/fp'\''")) | .commands |= map(gsub("--start_date DAILY"; "--start_date DAILY --end_date \($DATE)0000"))' execution.json > temp.json


      - name: Check and create AWS key pair
        run: |
          cd research_datastream/terraform_community
          if ! aws ec2 describe-key-pairs --key-names "actions_key" --query 'KeyPairs[0].KeyName' --output text 2>/dev/null; then 
            aws ec2 create-key-pair --key-name "actions_key" --query 'KeyName' --output text && echo "Key pair 'actions_key' created in AWS"; 
          else 
            echo "Key pair 'actions_key' already exists"; 
          fi

      - name: Start and monitor Step Functions execution
        run: |
          cd research_datastream/terraform_community
          execution_arn=$(aws stepfunctions start-execution --state-machine-arn $(cat ./sm_ARN.txt) --name fp_test_$(env TZ=US/Eastern date +'%Y%m%d%H%M%S') --input "file://temp.json" --region us-east-1 --query 'executionArn' --output text)
          echo "Execution ARN: $execution_arn"
          status="RUNNING"
          while [ "$status" != "SUCCEEDED" ]; do
            status=$(aws stepfunctions describe-execution --execution-arn "$execution_arn" --region us-east-1 --query 'status' --output text)
            echo "Current status: $status"
            if [ "$status" == "FAILED" ]; then
              echo "State machine execution failed!"
              exit 1
            fi
            sleep 5
          done
          echo "State machine execution succeeded!"


      - name: Verify output files
        run: |
          curl -fSs -o test_01.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_01.nc || { echo "Error: VPU_01 not found"; exit 1; }
          curl -fSs -o test_02.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_02.nc || { echo "Error: VPU_02 not found"; exit 1; }
          curl -fSs -o test_03W.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_03W.nc || { echo "Error: VPU_03W not found"; exit 1; }
          curl -fSs -o test_03S.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_03S.nc || { echo "Error: VPU_03S not found"; exit 1; }
          curl -fSs -o test_03N.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_03N.nc || { echo "Error: VPU_03N not found"; exit 1; }
          curl -fSs -o test_04.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_04.nc || { echo "Error: VPU_04 not found"; exit 1; }
          curl -fSs -o test_05.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_05.nc || { echo "Error: VPU_05 not found"; exit 1; }
          curl -fSs -o test_06.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_06.nc || { echo "Error: VPU_06 not found"; exit 1; }
          curl -fSs -o test_07.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_07.nc || { echo "Error: VPU_07 not found"; exit 1; }
          curl -fSs -o test_08.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_08.nc || { echo "Error: VPU_08 not found"; exit 1; }
          curl -fSs -o test_09.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_09.nc || { echo "Error: VPU_09 not found"; exit 1; }
          curl -fSs -o test_10L.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_10L.nc || { echo "Error: VPU_10L not found"; exit 1; }
          curl -fSs -o test_10U.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_10U.nc || { echo "Error: VPU_10U not found"; exit 1; }
          curl -fSs -o test_11.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_11.nc || { echo "Error: VPU_11 not found"; exit 1; }
          curl -fSs -o test_12.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_12.nc || { echo "Error: VPU_12 not found"; exit 1; }
          curl -fSs -o test_13.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_13.nc || { echo "Error: VPU_13 not found"; exit 1; }
          curl -fSs -o test_14.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_14.nc || { echo "Error: VPU_14 not found"; exit 1; }
          curl -fSs -o test_15.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_15.nc || { echo "Error: VPU_15 not found"; exit 1; }
          curl -fSs -o test_16.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_16.nc || { echo "Error: VPU_16 not found"; exit 1; }
          curl -fSs -o test_17.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_17.nc || { echo "Error: VPU_17 not found"; exit 1; }
          curl -fSs -o test_18.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/fp/ngen.t00z.short_range.forcing.f001_f018.VPU_18.nc || { echo "Error: VPU_18 not found"; exit 1; }

      - name: Debug - Check what files were created
        run: |
          echo "Checking what files were actually created..."
          aws s3 ls s3://ciroh-community-ngen-datastream/tests/ --recursive

      - name: Verify output files
        run: |
          echo "Checking if processing created any output files for FP..."
          file_list=$(aws s3 ls s3://ciroh-community-ngen-datastream/tests/short_range/fp/ --recursive 2>/dev/null || echo "")
          
          if [ -n "$file_list" ]; then
            echo "SUCCESS: FP processing completed successfully!"
            echo "Files created:"
            echo "$file_list"
          else
            echo "FAILED: No output files were created for FP"
            exit 1
          fi

      - name: Clean up
        if: always()
        run: |
          aws s3 rm --recursive s3://ciroh-community-ngen-datastream/tests/short_range/fp || echo "No file to delete"


  test-all-vpus:
    needs: [build-test-push-ami, get-ami-info]
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false  # Continue testing other VPUs even if one fails
      matrix:
        vpu: ["01", "02", "03N", "03S", "03W", "04", "05", "06", "07", "08", "09", "10L", "10U", "11", "12", "13", "14", "15", "16", "17", "18"]    
    env:
      VPU: ${{ matrix.vpu }}
      DATE: 20250801
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          pip install --upgrade awscli


      - name: Download and modify execution JSON
        run: |
          cd research_datastream/terraform_community
          
          # Use the AMI ID from the get-ami-info job
          ami_id="${{ needs.get-ami-info.outputs.ami_id }}"
          ami_name="datastream-${{ needs.build-test-push-ami.outputs.ds_ami_version }}"
          
          # Verify AMI ID is valid
          if [ "$ami_id" == "None" ] || [ "$ami_id" == "" ] || [ "$ami_id" == "null" ]; then
            echo " Invalid AMI ID: $ami_id"
            echo "Available AMIs:"
            aws ec2 describe-images --owners self --query 'Images[*].[Name,ImageId]' --output table --region us-east-1
            exit 1
          fi
          
          echo " Using AMI ID: $ami_id for AMI name: $ami_name"
          
          # Download the proven working execution JSON from S3
          aws s3 cp s3://ciroh-community-ngen-datastream/v2.2/ngen.${{ env.DATE }}/short_range/00/VPU_16/datastream-metadata/execution.json execution.json
          
          # Replace the working AMI with custom one
          jq --arg ami "$ami_id" '.instance_parameters.ImageId = $ami' execution.json > execution_updated.json
          mv execution_updated.json execution.json
          echo $ami_id
          # Replace Docker image tags with specific versions
          sed -i "s|DS_TAG=1.0.1|DS_TAG=${{ needs.build-test-push-ami.outputs.ds_tag }}|g" execution.json
          sed -i "s|NGIAB_TAG=v0.0.0|NGIAB_TAG=${{ needs.build-test-push-ami.outputs.ngiab_tag }}|g" execution.json
          cat execution.json

          # Show key info (avoid dumping entire JSON to logs)
          echo "AMI replacement completed for VPU ${{ env.VPU }}"
          echo "New AMI ID: $(jq -r '.instance_parameters.ImageId' execution.json)"
          
          # Apply jq transformations for this specific VPU
          jq --arg DATE "${{ env.DATE }}" --arg VPU "${{ env.VPU }}" 'del(.instance_parameters.MaxCount, .instance_parameters.MinCount, .instance_parameters.InstanceId, .t0, .ii_s3_object_checked, .retry_attempt, .region) | .instance_parameters.TagSpecifications = [{"ResourceType": "instance", "Tags": [{"Key": "Project", "Value": "fp_test_git_actions_vpu_\($VPU)"}, {"Key": "AMI_Version", "Value": "datastream-${{ needs.build-test-push-ami.outputs.ds_ami_version }}"}]}] | .commands |= map(sub("--S3_PREFIX(?i) \\K[^ ]+"; "tests/short_range/VPU_\($VPU)'\''")) | .commands |= map(gsub("VPU_16"; "VPU_\($VPU)")) | .commands |= map(gsub("-s DAILY"; "-s DAILY -e \($DATE)0000"))' execution.json > temp.json


      - name: Check and create AWS key pair
        run: |
          cd research_datastream/terraform_community
          if ! aws ec2 describe-key-pairs --key-names "actions_key" --query 'KeyPairs[0].KeyName' --output text 2>/dev/null; then 
            aws ec2 create-key-pair --key-name "actions_key" --query 'KeyName' --output text && echo "Key pair 'actions_key' created in AWS"; 
          else 
            echo "Key pair 'actions_key' already exists"; 
          fi

      - name: Start and monitor Step Functions execution
        id: stepfunction
        run: |
          cd research_datastream/terraform_community
          execution_arn=$(aws stepfunctions start-execution --state-machine-arn $(cat ./sm_ARN.txt) --name VPU_${{ env.VPU }}_test_$(env TZ=US/Eastern date +'%Y%m%d%H%M%S') --input "file://temp.json" --region us-east-1 --query 'executionArn' --output text)
          echo "Execution ARN: $execution_arn"
          status="RUNNING"
          while [ "$status" != "SUCCEEDED" ]; do
            status=$(aws stepfunctions describe-execution --execution-arn "$execution_arn" --region us-east-1 --query 'status' --output text)
            echo "Current status: $status"
            if [ "$status" == "FAILED" ]; then
              echo "State machine execution failed for VPU ${{ env.VPU }}!"
              echo "stepfunction_failed=true" >> "$GITHUB_OUTPUT"
              exit 1
            fi
            sleep 5
          done
          echo "State machine execution succeeded for VPU ${{ env.VPU }}!"
          echo "stepfunction_failed=false" >> "$GITHUB_OUTPUT"

      - name: Verify output files
        if: matrix.vpu != '10U' && matrix.vpu != '17'
        run: |
          echo "Checking if processing created any output files for VPU ${{ env.VPU }}..."
          
          # Check if the directory exists and has files
          file_list=$(aws s3 ls s3://ciroh-community-ngen-datastream/tests/short_range/VPU_${{ env.VPU }}/ --recursive 2>/dev/null || echo "")
          
          if [ -n "$file_list" ]; then
            echo "SUCCESS: VPU ${{ env.VPU }} datastream processing completed successfully!"
            echo "Files created:"
            echo "$file_list"
            
            # Count files for summary
            file_count=$(echo "$file_list" | wc -l)
            echo "Total files/objects created for VPU ${{ env.VPU }}: $file_count"
          else
            echo "FAILED: No output files were created for VPU ${{ env.VPU }}"
            exit 1
          fi

      - name: Skip verification notice
        if: matrix.vpu == '10U' || matrix.vpu == '17'
        run: |
          echo "NOTICE: Skipping output file verification for VPU ${{ env.VPU }} (known to have issues)"
          echo "Step Function execution completed successfully, but output verification is bypassed"

    
      - name: Verify output files
        if: matrix.vpu != '10U' && matrix.vpu != '17'
        run: |
          curl -fSs -o test.txt https://ciroh-community-ngen-datastream.s3.amazonaws.com/tests/short_range/VPU_${{ env.VPU }}/merkdir.file || { echo "Error: File not found or request failed"; exit 1; }

      - name: Clean up
        if: always()
        run: |
          aws s3 rm --recursive s3://ciroh-community-ngen-datastream/tests/short_range/VPU_${{ env.VPU }} || echo "No files to delete for VPU ${{ env.VPU }}"



  # Summary job that runs after all VPUs complete
  test-summary:
    runs-on: ubuntu-latest
    needs: [build-test-push-ami, test-all-vpus, get-ami-info]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Check VPU Test Results and Handle AMI
        run: |
          echo "VPU Testing Summary:"
          echo "======================"
          echo "Date: $(date)"
          echo "Total VPUs Tested: 21"
          echo ""
          
          # Check if any VPU tests failed
          # GitHub Actions sets job results in the needs context
          any_failed=false
          
          # Parse the results from the matrix jobs
          echo "Checking individual VPU results..."
          
          # In GitHub Actions, when any job in needs.test-all-vpus fails, 
          # the overall needs.test-all-vpus.result will be 'failure'
          if [ "${{ needs.test-all-vpus.result }}" != "success" ]; then
            any_failed=true
            echo "One or more VPU tests failed"
          else
            echo "All VPU tests passed"
          fi
          
          echo " Check individual job logs above for detailed results"
          echo " All test files have been cleaned up from S3"
          
          # Set output for next step
          echo "any_failed=$any_failed" >> $GITHUB_ENV

      - name: Deregister AMI on any test failure
        if: env.any_failed == 'true'
        run: |
          echo " VPU tests failed - checking for custom AMI to deregister..."
          ami_id="${{ needs.get-ami-info.outputs.ami_id }}"
          ami_to_deregister=$ami_id
          
          if [ "$ami_to_deregister" != "None" ] && [ "$ami_to_deregister" != "" ] && [ "$ami_to_deregister" != "null" ]; then
            echo "Found AMI to deregister: $ami_to_deregister"
            
            ami_name=$(aws ec2 describe-images --image-ids $ami_to_deregister --query 'Images[0].Name' --output text --region us-east-1 2>/dev/null || echo "Unknown")
            echo "AMI Name: $ami_name"
            
            # Get snapshot IDs before deregistering
            echo "Getting snapshot IDs..."
            snapshot_ids=$(aws ec2 describe-images --image-ids $ami_to_deregister --query 'Images[0].BlockDeviceMappings[*].Ebs.SnapshotId' --output text --region us-east-1 2>/dev/null || echo "")
            
            if [ -n "$snapshot_ids" ]; then
              echo "Found snapshots: $snapshot_ids"
            fi
            
            # Deregister AMI
            echo "Deregistering AMI: $ami_to_deregister"
            aws ec2 deregister-image --image-id $ami_to_deregister --region us-east-1
            echo " AMI $ami_to_deregister deregistered successfully"
            
            # Delete associated snapshots
            if [ -n "$snapshot_ids" ]; then
              for snapshot_id in $snapshot_ids; do
                if [ "$snapshot_id" != "None" ] && [ "$snapshot_id" != "" ] && [ "$snapshot_id" != "null" ]; then
                  echo "Deleting snapshot: $snapshot_id"
                  aws ec2 delete-snapshot --snapshot-id $snapshot_id --region us-east-1
                  echo "Snapshot $snapshot_id deleted"
                fi
              done
            else
              echo "No snapshots found to delete"
            fi
            
            echo "  AMI cleanup completed due to test failures"
          else
            echo "No custom AMI found to deregister"
          fi

      - name: Final Summary
        run: |
          echo " Final Results:"
          
          if [ "${{ env.any_failed }}" == "true" ]; then
            echo " OVERALL RESULT: Some VPU tests failed"
            echo "  Custom AMI has been deregistered due to failures"
            echo " Review failed VPU logs above for troubleshooting"
            exit 1
          else
            echo " OVERALL RESULT: All VPU tests passed successfully"
            echo " Custom AMI is validated and preserved"
            echo " Ready for production deployment"
          fi
